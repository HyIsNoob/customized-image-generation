{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DreamBooth Training Notebook\n",
        "\n",
        "**Nguyễn Khang Hy (2352662)**\n",
        "\n",
        "Fine-tune Stable Diffusion với DreamBooth cho các phong cách nghệ thuật từ WikiArt.\n",
        "\n",
        "## Mục tiêu\n",
        "\n",
        "- Fine-tune DreamBooth cho 1-2 phong cách đại diện\n",
        "- Ghi lại thời gian train, kích thước checkpoint, GPU usage\n",
        "- So sánh với LoRA về chất lượng vs chi phí\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import shutil\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from diffusers import AutoencoderKL, StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import compute_snr\n",
        "from diffusers.utils import check_min_version\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cấu hình và tham số\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "STYLE_NAME = \"Contemporary_Realism\"\n",
        "UNIQUE_TOKEN = \"sks\"\n",
        "INSTANCE_PROMPT = f\"a {UNIQUE_TOKEN} style painting\"\n",
        "CLASS_PROMPT = \"a painting\"\n",
        "\n",
        "INSTANCE_DIR = f\"/kaggle/working/dreambooth/{STYLE_NAME}/instance_images\"\n",
        "CLASS_DIR = f\"/kaggle/working/dreambooth/{STYLE_NAME}/class_images\"\n",
        "OUTPUT_DIR = f\"/kaggle/working/dreambooth_checkpoints/{STYLE_NAME}\"\n",
        "\n",
        "CLASS_IMAGES_DATASET_PATH = None\n",
        "for possible_path in [\n",
        "    f\"/kaggle/input/priorimages/dreambooth/{STYLE_NAME}/class_images\",\n",
        "    f\"/kaggle/input/dreambooth-class-images/dreambooth/{STYLE_NAME}/class_images\",\n",
        "    f\"/kaggle/input/dreambooth/dreambooth/{STYLE_NAME}/class_images\",\n",
        "    f\"/kaggle/input/dreambooth-class-images/class_images\",\n",
        "]:\n",
        "    if os.path.exists(possible_path) and len([f for f in os.listdir(possible_path) if f.endswith(('.png', '.jpg', '.jpeg'))]) >= 200:\n",
        "        CLASS_IMAGES_DATASET_PATH = possible_path\n",
        "        print(f\"Tìm thấy class images dataset tại: {possible_path}\")\n",
        "        break\n",
        "\n",
        "# Tối ưu memory: Giảm resolution và tăng gradient accumulation\n",
        "RESOLUTION = 256  # Giảm xuống 256 để tiết kiệm memory tối đa\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 16  # Tăng lên 16 để mô phỏng batch size lớn hơn\n",
        "MAX_TRAIN_STEPS = 1000\n",
        "LEARNING_RATE = 5e-6\n",
        "PRIOR_LOSS_WEIGHT = 1.0\n",
        "MIXED_PRECISION = \"fp16\"\n",
        "GRADIENT_CHECKPOINTING = True\n",
        "SCALE_LR = False\n",
        "LR_SCHEDULER = \"constant\"\n",
        "LR_WARMUP_STEPS = 0\n",
        "SNR_GAMMA = None\n",
        "USE_8BIT_ADAM = False  # Không dùng được do bitsandbytes không tương thích với kaggle\n",
        "SEED = 2025\n",
        "\n",
        "# Chỉ có thể train attention layer vì phần cứng không cho phép\n",
        "TRAIN_ONLY_ATTENTION = True  \n",
        "\n",
        "# Memory optimization flags\n",
        "ENABLE_VAE_SLICING = True  # Chia VAE encoding thành các slice nhỏ hơn\n",
        "ENABLE_VAE_TILING = True  # Chia VAE thành các tile để xử lý ảnh lớn\n",
        "ENABLE_ATTENTION_SLICING = \"max\"  # Chia attention thành các slice\n",
        "ENABLE_XFORMERS = True  # Sử dụng xformers memory efficient attention\n",
        "ENABLE_CPU_OFFLOAD = True  # Offload VAE và text encoder sang CPU khi không dùng\n",
        "\n",
        "# CUDA memory management\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True  # Tối ưu cuDNN\n",
        " \n",
        "os.makedirs(INSTANCE_DIR, exist_ok=True)\n",
        "os.makedirs(CLASS_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Chuẩn bị dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikiart_path = \"/kaggle/input/wikiart\"\n",
        "style_path = os.path.join(wikiart_path, STYLE_NAME)\n",
        "\n",
        "if not os.path.exists(style_path):\n",
        "    print(f\"Style {STYLE_NAME} không tồn tại. Kiểm tra lại đường dẫn.\")\n",
        "    print(f\"Các style có sẵn: {os.listdir(wikiart_path)[:10]}\")\n",
        "else:\n",
        "    image_files = [f for f in os.listdir(style_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    print(f\"Tìm thấy {len(image_files)} ảnh trong {STYLE_NAME}\")\n",
        "    \n",
        "    selected_images = image_files[:20]\n",
        "    print(f\"Chọn {len(selected_images)} ảnh cho instance images\")\n",
        "    \n",
        "    for img_file in selected_images:\n",
        "        src = os.path.join(style_path, img_file)\n",
        "        dst = os.path.join(INSTANCE_DIR, img_file)\n",
        "        shutil.copy(src, dst)\n",
        "        \n",
        "        caption_file = os.path.splitext(dst)[0] + \".txt\"\n",
        "        with open(caption_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(INSTANCE_PROMPT)\n",
        "    \n",
        "    print(f\"Đã copy {len(selected_images)} instance images và tạo captions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate prior preservation images (class images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_CLASS_IMAGES = 200\n",
        "\n",
        "existing_images = [f for f in os.listdir(CLASS_DIR) if f.endswith(('.png', '.jpg', '.jpeg'))] if os.path.exists(CLASS_DIR) else []\n",
        "\n",
        "if len(existing_images) >= NUM_CLASS_IMAGES:\n",
        "    print(f\"Đã có {len(existing_images)} class images trong {CLASS_DIR}, skip generation\")\n",
        "elif CLASS_IMAGES_DATASET_PATH is not None:\n",
        "    print(f\"Copying class images từ dataset: {CLASS_IMAGES_DATASET_PATH}\")\n",
        "    dataset_images = [f for f in os.listdir(CLASS_IMAGES_DATASET_PATH) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    dataset_images = sorted(dataset_images)[:NUM_CLASS_IMAGES]\n",
        "    \n",
        "    for img_file in tqdm(dataset_images):\n",
        "        src = os.path.join(CLASS_IMAGES_DATASET_PATH, img_file)\n",
        "        dst = os.path.join(CLASS_DIR, img_file)\n",
        "        shutil.copy(src, dst)\n",
        "        \n",
        "        caption_file = os.path.splitext(dst)[0] + \".txt\"\n",
        "        if not os.path.exists(caption_file):\n",
        "            with open(caption_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(CLASS_PROMPT)\n",
        "    \n",
        "    print(f\"Đã copy {len(dataset_images)} class images từ dataset\")\n",
        "else:\n",
        "    print(f\"Generating {NUM_CLASS_IMAGES} class images...\")\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16 if MIXED_PRECISION == \"fp16\" else torch.float32,\n",
        "    )\n",
        "    pipeline = pipeline.to(\"cuda\")\n",
        "    \n",
        "    start_idx = len(existing_images)\n",
        "    for i in tqdm(range(start_idx, NUM_CLASS_IMAGES)):\n",
        "        image = pipeline(CLASS_PROMPT, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
        "        image.save(os.path.join(CLASS_DIR, f\"{i:04d}.png\"))\n",
        "        \n",
        "        caption_file = os.path.join(CLASS_DIR, f\"{i:04d}.txt\")\n",
        "        with open(caption_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(CLASS_PROMPT)\n",
        "    \n",
        "    del pipeline\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Đã generate {NUM_CLASS_IMAGES - start_idx} class images mới (tổng: {NUM_CLASS_IMAGES})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load models và tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\")\n",
        "vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\")\n",
        "unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\")\n",
        "\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
        "\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Chỉ train attention layers để giảm memory (optimizer state sẽ nhỏ hơn)\n",
        "if TRAIN_ONLY_ATTENTION:\n",
        "    # Freeze tất cả parameters trước\n",
        "    unet.requires_grad_(False)\n",
        "    # Chỉ enable gradient cho attention layers\n",
        "    for name, module in unet.named_modules():\n",
        "        if \"attn\" in name or \"attention\" in name:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = True\n",
        "    print(\"Chỉ train attention layers (giảm ~70% parameters cần train)\")\n",
        "else:\n",
        "    unet.requires_grad_(True)\n",
        "    print(\"Train toàn bộ UNet\")\n",
        "\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "    unet.enable_gradient_checkpointing()\n",
        "\n",
        "# UNet phải là float32 để gradients là float32\n",
        "# Autocast sẽ tự động convert sang float16 trong forward pass\n",
        "# VAE và text_encoder có thể dùng float16 vì không train\n",
        "vae_dtype = torch.float16 if MIXED_PRECISION == \"fp16\" else torch.float32\n",
        "unet_dtype = torch.float32  # Luôn dùng float32 cho UNet để tránh lỗi gradient scaling\n",
        "\n",
        "# Áp dụng memory optimizations\n",
        "if ENABLE_VAE_SLICING:\n",
        "    vae.enable_slicing()\n",
        "    print(\"VAE slicing enabled\")\n",
        "\n",
        "if ENABLE_VAE_TILING:\n",
        "    vae.enable_tiling()\n",
        "    print(\"VAE tiling enabled\")\n",
        "\n",
        "if ENABLE_ATTENTION_SLICING:\n",
        "    unet.set_attention_slice(ENABLE_ATTENTION_SLICING)\n",
        "    print(f\"Attention slicing enabled: {ENABLE_ATTENTION_SLICING}\")\n",
        "\n",
        "if ENABLE_XFORMERS:\n",
        "    try:\n",
        "        unet.enable_xformers_memory_efficient_attention()\n",
        "        print(\"XFormers memory efficient attention enabled\")\n",
        "    except Exception as e:\n",
        "        print(f\"XFormers không khả dụng: {e}\")\n",
        "\n",
        "if ENABLE_CPU_OFFLOAD:\n",
        "    vae.to(\"cpu\")\n",
        "    text_encoder.to(\"cpu\")\n",
        "    print(\"✓ CPU offloading enabled cho VAE và Text Encoder\")\n",
        "else:\n",
        "    vae.to(\"cuda\", dtype=vae_dtype)\n",
        "    text_encoder.to(\"cuda\", dtype=vae_dtype)\n",
        "\n",
        "unet.to(\"cuda\", dtype=unet_dtype)\n",
        "\n",
        "print(f\"VAE dtype: {vae_dtype} (device: {'CPU' if ENABLE_CPU_OFFLOAD else 'CUDA'})\")\n",
        "print(f\"Text Encoder dtype: {vae_dtype} (device: {'CPU' if ENABLE_CPU_OFFLOAD else 'CUDA'})\")\n",
        "print(f\"UNet dtype: {unet_dtype} (sẽ tự động convert sang float16 trong forward pass)\")\n",
        "print(\"Đã load models với memory optimizations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup Accelerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_config = ProjectConfiguration(project_dir=OUTPUT_DIR)\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    mixed_precision=MIXED_PRECISION,\n",
        "    project_config=project_config,\n",
        ")\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    accelerator.init_trackers(\"dreambooth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Chuẩn bị dataset và dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(self, instance_data_root, class_data_root, tokenizer, size=512, repeats=1):\n",
        "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
        "        self.instance_images_path = [p for p in self.instance_images_path if p.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = INSTANCE_PROMPT\n",
        "        self._length = self.num_instance_images * repeats\n",
        "\n",
        "        if class_data_root is not None:\n",
        "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
        "            self.class_images_path = [p for p in self.class_images_path if p.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_class_images, self.num_instance_images) * repeats\n",
        "        else:\n",
        "            self.class_images_path = None\n",
        "\n",
        "        self.size = size\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.image_transforms = transforms.Compose([\n",
        "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.CenterCrop(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_images_path:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                CLASS_PROMPT,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                return_tensors=\"pt\",\n",
        "            ).input_ids\n",
        "\n",
        "        return example\n",
        "\n",
        "def collate_fn(examples):\n",
        "    batch = {\n",
        "        \"instance_images\": torch.stack([example[\"instance_images\"] for example in examples]),\n",
        "        \"instance_prompt_ids\": torch.stack([example[\"instance_prompt_ids\"] for example in examples]).squeeze(1),\n",
        "    }\n",
        "    if \"class_images\" in examples[0]:\n",
        "        batch[\"class_images\"] = torch.stack([example[\"class_images\"] for example in examples])\n",
        "        batch[\"class_prompt_ids\"] = torch.stack([example[\"class_prompt_ids\"] for example in examples]).squeeze(1)\n",
        "    return batch\n",
        "\n",
        "train_dataset = DreamBoothDataset(\n",
        "    instance_data_root=INSTANCE_DIR,\n",
        "    class_data_root=CLASS_DIR,\n",
        "    tokenizer=tokenizer,\n",
        "    size=RESOLUTION,\n",
        "    repeats=1,\n",
        ")\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(train_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Setup optimizer và scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_8BIT_ADAM:\n",
        "    import bitsandbytes as bnb\n",
        "    optimizer_class = bnb.optim.AdamW8bit\n",
        "else:\n",
        "    optimizer_class = torch.optim.AdamW\n",
        "\n",
        "learning_rate = LEARNING_RATE\n",
        "if SCALE_LR:\n",
        "    learning_rate = LEARNING_RATE * GRADIENT_ACCUMULATION_STEPS * TRAIN_BATCH_SIZE * accelerator.num_processes\n",
        "\n",
        "# optimize các parameters có requires_grad=True để giảm optimizer state\n",
        "trainable_params = [p for p in unet.parameters() if p.requires_grad]\n",
        "num_trainable = sum(p.numel() for p in trainable_params)\n",
        "num_total = sum(p.numel() for p in unet.parameters())\n",
        "print(f\"Trainable parameters: {num_trainable:,} / {num_total:,} ({100*num_trainable/num_total:.1f}%)\")\n",
        "\n",
        "optimizer = optimizer_class(\n",
        "    trainable_params,  # optimize trainable parameters\n",
        "    lr=learning_rate,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-2,\n",
        "    eps=1e-08,\n",
        ")\n",
        "\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / GRADIENT_ACCUMULATION_STEPS)\n",
        "max_train_steps = MAX_TRAIN_STEPS\n",
        "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    LR_SCHEDULER,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=LR_WARMUP_STEPS * GRADIENT_ACCUMULATION_STEPS,\n",
        "    num_training_steps=max_train_steps * GRADIENT_ACCUMULATION_STEPS,\n",
        ")\n",
        "\n",
        "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    unet, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "if not ENABLE_CPU_OFFLOAD:\n",
        "    text_encoder.to(accelerator.device, dtype=vae_dtype)\n",
        "    vae.to(accelerator.device, dtype=vae_dtype)\n",
        "\n",
        "print(f\"Max train steps: {max_train_steps}\")\n",
        "print(f\"Num epochs: {num_train_epochs}\")\n",
        "print(f\"Mixed precision: {MIXED_PRECISION}\")\n",
        "print(f\"CPU offloading: {ENABLE_CPU_OFFLOAD}\")\n",
        "print(f\"UNet sẽ được autocast sang float16 trong forward pass\" if MIXED_PRECISION == \"fp16\" else \"UNet dùng float32\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "progress_bar.set_description(\"Steps\")\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    unet.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        with accelerator.accumulate(unet):\n",
        "            with accelerator.autocast():\n",
        "                pixel_values = batch[\"instance_images\"].to(accelerator.device, dtype=vae_dtype)\n",
        "                input_ids = batch[\"instance_prompt_ids\"].to(accelerator.device)\n",
        "\n",
        "                # CPU offloading: Move VAE lên GPU khi encode\n",
        "                if ENABLE_CPU_OFFLOAD:\n",
        "                    vae.to(accelerator.device, dtype=vae_dtype)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    latents = vae.encode(pixel_values).latent_dist.sample()\n",
        "                    latents = latents * vae.config.scaling_factor\n",
        "                \n",
        "                # Move VAE về CPU sau khi encode\n",
        "                if ENABLE_CPU_OFFLOAD:\n",
        "                    vae.to(\"cpu\")\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # CPU offloading: Move text encoder lên GPU khi encode\n",
        "                if ENABLE_CPU_OFFLOAD:\n",
        "                    text_encoder.to(accelerator.device, dtype=vae_dtype)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    encoder_hidden_states = text_encoder(input_ids.squeeze(1))[0]\n",
        "                \n",
        "                # Move text encoder về CPU sau khi encode\n",
        "                if ENABLE_CPU_OFFLOAD:\n",
        "                    text_encoder.to(\"cpu\")\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                if SNR_GAMMA is None:\n",
        "                    loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "                else:\n",
        "                    snr = compute_snr(noise_scheduler, timesteps)\n",
        "                    mse_loss_weights = (\n",
        "                        torch.stack([snr, SNR_GAMMA * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\n",
        "                    )\n",
        "                    loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
        "                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
        "                    loss = loss.mean()\n",
        "\n",
        "                if \"class_images\" in batch:\n",
        "                    class_pixel_values = batch[\"class_images\"].to(accelerator.device, dtype=vae_dtype)\n",
        "                    class_input_ids = batch[\"class_prompt_ids\"].to(accelerator.device)\n",
        "\n",
        "                    # CPU offloading: Move VAE lên GPU khi encode class images\n",
        "                    if ENABLE_CPU_OFFLOAD:\n",
        "                        vae.to(accelerator.device, dtype=vae_dtype)\n",
        "                    \n",
        "                    with torch.no_grad():\n",
        "                        class_latents = vae.encode(class_pixel_values).latent_dist.sample()\n",
        "                        class_latents = class_latents * vae.config.scaling_factor\n",
        "                    \n",
        "                    # Move VAE về CPU sau khi encode\n",
        "                    if ENABLE_CPU_OFFLOAD:\n",
        "                        vae.to(\"cpu\")\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    class_noise = torch.randn_like(class_latents)\n",
        "                    class_timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=class_latents.device).long()\n",
        "\n",
        "                    class_noisy_latents = noise_scheduler.add_noise(class_latents, class_noise, class_timesteps)\n",
        "\n",
        "                    # CPU offloading: Move text encoder lên GPU khi encode class prompts\n",
        "                    if ENABLE_CPU_OFFLOAD:\n",
        "                        text_encoder.to(accelerator.device, dtype=vae_dtype)\n",
        "                    \n",
        "                    with torch.no_grad():\n",
        "                        class_encoder_hidden_states = text_encoder(class_input_ids.squeeze(1))[0]\n",
        "                    \n",
        "                    # Move text encoder về CPU sau khi encode\n",
        "                    if ENABLE_CPU_OFFLOAD:\n",
        "                        text_encoder.to(\"cpu\")\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                    class_model_pred = unet(class_noisy_latents, class_timesteps, class_encoder_hidden_states).sample\n",
        "\n",
        "                    if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                        class_target = class_noise\n",
        "                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                        class_target = noise_scheduler.get_velocity(class_latents, class_noise, class_timesteps)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                    if SNR_GAMMA is None:\n",
        "                        class_loss = torch.nn.functional.mse_loss(class_model_pred.float(), class_target.float(), reduction=\"mean\")\n",
        "                    else:\n",
        "                        class_snr = compute_snr(noise_scheduler, class_timesteps)\n",
        "                        class_mse_loss_weights = (\n",
        "                            torch.stack([class_snr, SNR_GAMMA * torch.ones_like(class_timesteps)], dim=1).min(dim=1)[0] / class_snr\n",
        "                        )\n",
        "                        class_loss = torch.nn.functional.mse_loss(class_model_pred.float(), class_target.float(), reduction=\"none\")\n",
        "                        class_loss = class_loss.mean(dim=list(range(1, len(class_loss.shape)))) * class_mse_loss_weights\n",
        "                        class_loss = class_loss.mean()\n",
        "\n",
        "                    loss = loss + PRIOR_LOSS_WEIGHT * class_loss\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                if MIXED_PRECISION != \"fp16\":\n",
        "                    # Chỉ clip gradient của trainable parameters\n",
        "                    trainable_params = [p for p in unet.parameters() if p.requires_grad]\n",
        "                    accelerator.clip_grad_norm_(trainable_params, 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Clear cache sau mỗi optimizer step để giải phóng memory\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "\n",
        "            # Memory monitoring và logging\n",
        "            if global_step % 100 == 0 and accelerator.is_main_process:\n",
        "                # Log loss\n",
        "                accelerator.log({\"loss\": loss.detach().item()}, step=global_step)\n",
        "                \n",
        "                # Log memory usage\n",
        "                if torch.cuda.is_available():\n",
        "                    memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "                    memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n",
        "                    accelerator.log({\n",
        "                        \"memory_allocated_gb\": memory_allocated,\n",
        "                        \"memory_reserved_gb\": memory_reserved\n",
        "                    }, step=global_step)\n",
        "                    print(f\"Step {global_step}: Loss={loss.detach().item():.4f}, \"\n",
        "                          f\"Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n",
        "                \n",
        "                # Clear cache định kỳ\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            if global_step >= max_train_steps:\n",
        "                break\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    unet = accelerator.unwrap_model(unet)\n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        text_encoder=text_encoder,\n",
        "        vae=vae,\n",
        "        unet=unet,\n",
        "    )\n",
        "    pipeline.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"Đã lưu checkpoint tại {OUTPUT_DIR}\")\n",
        "\n",
        "accelerator.end_training()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Ghi lại thông tin training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_info = {\n",
        "    \"style_name\": STYLE_NAME,\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"max_train_steps\": MAX_TRAIN_STEPS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
        "    \"mixed_precision\": MIXED_PRECISION,\n",
        "    \"resolution\": RESOLUTION,\n",
        "    \"num_instance_images\": len(os.listdir(INSTANCE_DIR)) // 2,\n",
        "    \"num_class_images\": len(os.listdir(CLASS_DIR)) // 2,\n",
        "    \"checkpoint_size_mb\": sum(f.stat().st_size for f in Path(OUTPUT_DIR).rglob('*') if f.is_file()) / (1024 * 1024),\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "}\n",
        "\n",
        "info_path = os.path.join(OUTPUT_DIR, \"training_info.json\")\n",
        "with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_info, f, indent=2)\n",
        "\n",
        "print(\"Training info:\")\n",
        "for key, value in training_info.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
