{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - LoRA vs Baseline (FINAL)\n",
    "\n",
    "Đánh giá LoRA (img2img) bằng bộ metrics CLIP cuối cùng để đo content preservation và style quality.\n",
    "\n",
    "**Metrics (FINAL):**\n",
    "1. **CLIP-content (chính)**: `1 - cos_sim(clip(output), clip(content))` – đo semantic content preservation.\n",
    "2. **Style Strength Score (phụ)**: `CLIP-content / baseline_CLIP-content` – đo mức độ thay đổi style so với baseline.\n",
    "3. **CLIP-style (chính)**: `1 - cos_sim(clip(output), clip(style_reference))` – đo mức độ giống style reference.\n",
    "\n",
    "**Models đánh giá:**\n",
    "- Baseline: SD v1.5 gốc (img2img)\n",
    "- LoRA: Action_painting, Analytical_Cubism, Contemporary_Realism, New_Realism, Synthetic_Cubism (5 styles)\n",
    "\n",
    "Notebook này chỉ chạy LoRA để tránh OOM. Chạy `04b_Evaluation_Metrics_DreamBooth_TI_FINAL.ipynb` (sẽ tạo sau) để đánh giá DreamBooth và TI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q lpips torchmetrics[image] torch-fidelity protobuf==3.20.3 ftfy regex git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuFFT.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuDNN.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuBLAS.*\")\n",
    "\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "else:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from torchvision import transforms as T\n",
    "import clip\n",
    "\n",
    "LORA_STYLES = [\"Action_painting\", \"Analytical_Cubism\", \"Contemporary_Realism\", \"New_Realism\", \"Synthetic_Cubism\"]\n",
    "RESOLUTION = 256\n",
    "NUM_SAMPLES = 20\n",
    "IMG2IMG_STRENGTH = 0.5\n",
    "IMG2IMG_GUIDANCE = 7.5\n",
    "MAX_CONTENT_SAMPLES = 8\n",
    "NUM_STYLE_IMAGES = 10\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class StableDiffusionTransform:\n",
    "    def __init__(self, size=256, center_crop=True):\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(size),\n",
    "            T.CenterCrop(size) if center_crop else T.RandomCrop(size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    def __call__(self, img):\n",
    "        return self.transform(img)\n",
    "\n",
    "def load_style_images(style_name: str, num_images: int = NUM_STYLE_IMAGES) -> List[Image.Image]:\n",
    "    style_dir = WIKIART_DIR / style_name\n",
    "    if not style_dir.exists():\n",
    "        print(f\"Warning: Style directory not found: {style_dir}\")\n",
    "        return []\n",
    "    \n",
    "    image_files = sorted(list(style_dir.glob(\"*.jpg\")) + list(style_dir.glob(\"*.png\")))\n",
    "    if not image_files:\n",
    "        print(f\"Warning: No images found in {style_dir}\")\n",
    "        return []\n",
    "    \n",
    "    selected_files = image_files[:min(num_images, len(image_files))]\n",
    "    style_images = [Image.open(f).convert(\"RGB\").resize((RESOLUTION, RESOLUTION), Image.BILINEAR) \n",
    "                    for f in selected_files]\n",
    "    \n",
    "    return style_images\n",
    "\n",
    "def compute_clip_distance(img_a, img_b):\n",
    "    with torch.no_grad():\n",
    "        tensor_a = clip_preprocess(img_a).unsqueeze(0).to(device)\n",
    "        tensor_b = clip_preprocess(img_b).unsqueeze(0).to(device)\n",
    "        \n",
    "        feat_a = clip_model.encode_image(tensor_a)\n",
    "        feat_b = clip_model.encode_image(tensor_b)\n",
    "        \n",
    "        feat_a = feat_a / feat_a.norm(dim=-1, keepdim=True)\n",
    "        feat_b = feat_b / feat_b.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        cos_sim = (feat_a * feat_b).sum(dim=-1)\n",
    "        return (1.0 - cos_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "TORCH_DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "def setup_pipeline(pipeline):\n",
    "    if torch.cuda.is_available():\n",
    "        pipeline = pipeline.to(device)\n",
    "    pipeline.enable_attention_slicing()\n",
    "    return pipeline\n",
    "\n",
    "RESULTS_DIR = Path(\"/kaggle/working/results/metrics\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_CACHE_DIR = Path(\"/kaggle/input/styleandcontent\")\n",
    "DATASET_CONTENT_PATH = DATASET_CACHE_DIR / \"content_paths.json\"\n",
    "DATASET_STYLE_PATH = DATASET_CACHE_DIR / \"style_paths.json\"\n",
    "\n",
    "LOCAL_CONTENT_CACHE = RESULTS_DIR / \"content_paths.json\"\n",
    "LOCAL_STYLE_CACHE = RESULTS_DIR / \"style_paths.json\"\n",
    "\n",
    "CONTENT_PATHS_FILE = DATASET_CONTENT_PATH if DATASET_CONTENT_PATH.exists() else LOCAL_CONTENT_CACHE\n",
    "STYLE_PATHS_FILE = DATASET_STYLE_PATH if DATASET_STYLE_PATH.exists() else LOCAL_STYLE_CACHE\n",
    "\n",
    "CONTENT_PATHS_WRITE_TARGET = LOCAL_CONTENT_CACHE\n",
    "STYLE_PATHS_WRITE_TARGET = LOCAL_STYLE_CACHE\n",
    "\n",
    "LORA_DATASET_MAP = {\n",
    "    \"Action_painting\": \"/kaggle/input/dts-lora-actionpainting\",\n",
    "    \"Analytical_Cubism\": \"/kaggle/input/dts-lora-analyticalcubism\",\n",
    "    \"Contemporary_Realism\": \"/kaggle/input/dts-lora-contemporaryrealism\",\n",
    "    \"New_Realism\": \"/kaggle/input/dts-lora-newrealism\",\n",
    "    \"Synthetic_Cubism\": \"/kaggle/input/dts-lora-syntheticcubism\",\n",
    "}\n",
    "\n",
    "CONTENT_IMAGES_DIRS = [\n",
    "    \"/kaggle/input/coco-2017-dataset/coco2017/val2017\",\n",
    "    \"/kaggle/input/coco2017/val2017\",\n",
    "]\n",
    "\n",
    "WIKIART_DIR = Path(\"/kaggle/input/wikiart\")\n",
    "if not WIKIART_DIR.exists():\n",
    "    WIKIART_DIR = Path(\"/kaggle/input/steubk/wikiart\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading CLIP model...\")\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "print(\"CLIP loaded\")\n",
    "\n",
    "print(\"Loading baseline img2img model...\")\n",
    "baseline_pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "baseline_pipeline = setup_pipeline(baseline_pipeline)\n",
    "print(\"Baseline loaded\")\n",
    "\n",
    "lora_pipelines = {}\n",
    "\n",
    "for style_name in LORA_STYLES:\n",
    "    if style_name not in LORA_DATASET_MAP:\n",
    "        print(f\"No LoRA dataset path mapped for {style_name}\")\n",
    "        continue\n",
    "    \n",
    "    lora_base = Path(LORA_DATASET_MAP[style_name])\n",
    "    lora_weights_path = lora_base / \"lora_models\" / style_name / \"pytorch_lora_weights.safetensors\"\n",
    "    \n",
    "    if not lora_base.exists():\n",
    "        print(f\"LoRA dataset not found: {lora_base}\")\n",
    "        continue\n",
    "    \n",
    "    if not lora_weights_path.exists():\n",
    "        print(f\"LoRA weights not found: {lora_weights_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Loading LoRA {style_name} from {lora_base}...\")\n",
    "    try:\n",
    "        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            torch_dtype=TORCH_DTYPE,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False,\n",
    "        )\n",
    "        pipeline.load_lora_weights(str(lora_weights_path.parent))\n",
    "        pipeline = setup_pipeline(pipeline)\n",
    "        lora_pipelines[style_name] = pipeline\n",
    "        print(f\"LoRA {style_name} loaded\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LoRA {style_name}: {e}\")\n",
    "\n",
    "models = {\n",
    "    \"baseline\": baseline_pipeline,\n",
    "    **{f\"lora_{s.lower().replace('_', '')}\": lora_pipelines[s] \n",
    "       for s in LORA_STYLES if s in lora_pipelines}\n",
    "}\n",
    "print(f\"\\nModels loaded: {list(models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_or_create_content_paths(max_items: int) -> List[str]:\n",
    "    paths: List[str] = []\n",
    "    if CONTENT_PATHS_FILE.exists():\n",
    "        try:\n",
    "            with open(CONTENT_PATHS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                stored = json.load(f)\n",
    "            candidate = stored.get(\"content_paths\", [])\n",
    "            paths = [p for p in candidate if Path(p).exists()]\n",
    "            if len(paths) < max_items:\n",
    "                print(\"Stored content paths are incomplete. Regenerating.\")\n",
    "                paths = []\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to read {CONTENT_PATHS_FILE}: {exc}. Regenerating content paths.\")\n",
    "            paths = []\n",
    "    if not paths:\n",
    "        print(\"Generating deterministic content paths...\")\n",
    "        for base_dir in CONTENT_IMAGES_DIRS:\n",
    "            content_dir = Path(base_dir)\n",
    "            if not (content_dir.exists() and content_dir.is_dir()):\n",
    "                continue\n",
    "            files = sorted(list(content_dir.glob(\"*.jpg\")) + list(content_dir.glob(\"*.png\")))\n",
    "            if len(files) >= max_items:\n",
    "                paths = [str(p.resolve()) for p in files[:max_items]]\n",
    "                with open(CONTENT_PATHS_WRITE_TARGET, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump({\"content_paths\": paths}, f, indent=2)\n",
    "                print(f\"Saved {len(paths)} content paths to {CONTENT_PATHS_WRITE_TARGET}\")\n",
    "                break\n",
    "        if len(paths) < max_items:\n",
    "            raise RuntimeError(\"Unable to prepare deterministic content paths. Check CONTENT_IMAGES_DIRS.\")\n",
    "    return paths[:max_items]\n",
    "\n",
    "content_paths = _load_or_create_content_paths(MAX_CONTENT_SAMPLES)\n",
    "content_images = [Image.open(Path(p)).convert(\"RGB\") for p in content_paths]\n",
    "transform = StableDiffusionTransform(size=RESOLUTION, center_crop=True)\n",
    "content_subset = [img.resize((RESOLUTION, RESOLUTION), Image.BILINEAR) for img in content_images]\n",
    "\n",
    "print(f\"Using {len(content_subset)} content images from deterministic list.\")\n",
    "\n",
    "style_paths_data: Dict[str, List[str]] = {}\n",
    "if STYLE_PATHS_FILE.exists():\n",
    "    try:\n",
    "        with open(STYLE_PATHS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            stored_styles = json.load(f)\n",
    "        style_paths_data = stored_styles.get(\"styles\", {})\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to read {STYLE_PATHS_FILE}: {exc}. Regenerating style paths.\")\n",
    "        style_paths_data = {}\n",
    "\n",
    "style_paths_updated = False\n",
    "style_images_map: Dict[str, List[Image.Image]] = {}\n",
    "print(\"\\nLoading style images from WikiArt (deterministic)...\")\n",
    "for style_name in LORA_STYLES:\n",
    "    stored_paths = [p for p in style_paths_data.get(style_name, []) if Path(p).exists()]\n",
    "    if len(stored_paths) >= NUM_STYLE_IMAGES:\n",
    "        selected_paths = stored_paths[:NUM_STYLE_IMAGES]\n",
    "    else:\n",
    "        style_dir = WIKIART_DIR / style_name\n",
    "        if not style_dir.exists():\n",
    "            print(f\"  {style_name}: directory not found {style_dir}\")\n",
    "            continue\n",
    "        files = sorted(list(style_dir.glob(\"*.jpg\")) + list(style_dir.glob(\"*.png\")))\n",
    "        if not files:\n",
    "            print(f\"  {style_name}: no images found in {style_dir}\")\n",
    "            continue\n",
    "        selected_paths = [str(p.resolve()) for p in files[:NUM_STYLE_IMAGES]]\n",
    "        style_paths_data[style_name] = selected_paths\n",
    "        style_paths_updated = True\n",
    "    style_images = [Image.open(Path(p)).convert(\"RGB\").resize((RESOLUTION, RESOLUTION), Image.BILINEAR)\n",
    "                    for p in selected_paths]\n",
    "    style_images_map[style_name] = style_images\n",
    "    print(f\"  {style_name}: {len(style_images)} images\")\n",
    "\n",
    "if style_paths_updated:\n",
    "    with open(STYLE_PATHS_WRITE_TARGET, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"styles\": style_paths_data}, f, indent=2)\n",
    "\n",
    "if not style_images_map:\n",
    "    print(\"Warning: No style images loaded. CLIP-style metrics will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_img2img(pipeline, prompt, inputs):\n",
    "    outputs = []\n",
    "    for idx, img in enumerate(inputs):\n",
    "        generator = torch.Generator(device=device).manual_seed(SEED + idx)\n",
    "        result = pipeline(\n",
    "            prompt=prompt,\n",
    "            image=img,\n",
    "            strength=IMG2IMG_STRENGTH,\n",
    "            guidance_scale=IMG2IMG_GUIDANCE,\n",
    "            num_inference_steps=50,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "        outputs.append(result)\n",
    "    return outputs\n",
    "\n",
    "style_prompts = {\n",
    "    \"baseline\": \"a realistic depiction of the same scene\",\n",
    "}\n",
    "\n",
    "for style_name in LORA_STYLES:\n",
    "    style_prompts[f\"lora_{style_name.lower().replace('_', '')}\"] = (\n",
    "        f\"a {style_name.replace('_', ' ').lower()} painting of the scene\"\n",
    "    )\n",
    "\n",
    "generated_samples = {}\n",
    "\n",
    "print(\"Generating baseline img2img samples...\")\n",
    "baseline_samples = run_img2img(\n",
    "    baseline_pipeline,\n",
    "    style_prompts[\"baseline\"],\n",
    "    content_subset,\n",
    ")\n",
    "generated_samples[\"baseline\"] = baseline_samples\n",
    "print(f\"Baseline: {len(baseline_samples)} samples\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for style_name in LORA_STYLES:\n",
    "    model_key = f\"lora_{style_name.lower().replace('_', '')}\"\n",
    "    if style_name in lora_pipelines:\n",
    "        pipeline = lora_pipelines[style_name]\n",
    "        print(f\"\\nGenerating samples from LoRA {style_name}...\")\n",
    "        style_samples = run_img2img(\n",
    "            pipeline,\n",
    "            style_prompts[model_key],\n",
    "            content_subset,\n",
    "        )\n",
    "        generated_samples[model_key] = style_samples\n",
    "        print(f\"LoRA {style_name}: {len(style_samples)} samples\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nTotal generated samples: {sum(len(v) for v in generated_samples.values())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_model(\n",
    "    model_name: str,\n",
    "    generated_images: List[Image.Image],\n",
    "    content_images_list: List[Image.Image],\n",
    "    style_images_list: Optional[List[Image.Image]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    clip_content_scores = []\n",
    "    for gen_img, content_img in zip(generated_images, content_images_list[:len(generated_images)]):\n",
    "        clip_score = compute_clip_distance(gen_img, content_img)\n",
    "        clip_content_scores.append(clip_score)\n",
    "    clip_content_mean = np.mean(clip_content_scores)\n",
    "    clip_content_std = np.std(clip_content_scores)\n",
    "    \n",
    "    metrics = {\n",
    "        \"clip_content\": clip_content_mean,\n",
    "        \"clip_content_std\": clip_content_std,\n",
    "    }\n",
    "    \n",
    "    if style_images_list and len(style_images_list) > 0:\n",
    "        clip_style_scores = []\n",
    "        for gen_img, style_img in zip(generated_images, style_images_list[:len(generated_images)]):\n",
    "            clip_style = compute_clip_distance(gen_img, style_img)\n",
    "            clip_style_scores.append(clip_style)\n",
    "        if clip_style_scores:\n",
    "            metrics[\"clip_style\"] = np.mean(clip_style_scores)\n",
    "            metrics[\"clip_style_std\"] = np.std(clip_style_scores)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, samples in generated_samples.items():\n",
    "    print(f\"\\nComputing metrics for {model_name}...\")\n",
    "    \n",
    "    style_images_for_model = None\n",
    "    if model_name != \"baseline\":\n",
    "        style_name = None\n",
    "        for s in LORA_STYLES:\n",
    "            if s.lower().replace('_', '') in model_name.lower():\n",
    "                style_name = s\n",
    "                break\n",
    "        if style_name and style_name in style_images_map:\n",
    "            style_images_for_model = style_images_map[style_name]\n",
    "    \n",
    "    metrics = compute_metrics_for_model(\n",
    "        model_name,\n",
    "        samples,\n",
    "        content_subset,\n",
    "        style_images_for_model,\n",
    "    )\n",
    "    all_metrics[model_name] = metrics\n",
    "    print(f\"  CLIP-content: {metrics['clip_content']:.4f} ± {metrics['clip_content_std']:.4f}\")\n",
    "    if 'clip_style' in metrics:\n",
    "        print(f\"  CLIP-style: {metrics['clip_style']:.4f} ± {metrics['clip_style_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_clip_value = all_metrics.get(\"baseline\", {}).get(\"clip_content\", None)\n",
    "if baseline_clip_value is not None and baseline_clip_value > 0:\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        metrics[\"style_strength\"] = metrics[\"clip_content\"] / baseline_clip_value\n",
    "else:\n",
    "    for metrics in all_metrics.values():\n",
    "        metrics[\"style_strength\"] = np.nan\n",
    "\n",
    "if \"baseline\" in all_metrics:\n",
    "    all_metrics[\"baseline\"][\"style_strength\"] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_metrics).T\n",
    "results_df.index.name = \"model\"\n",
    "results_df = results_df.reset_index()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS - LoRA (FINAL)\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "results_path = RESULTS_DIR / \"lora_metrics_final.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "\n",
    "json_path = RESULTS_DIR / \"lora_metrics_final.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "print(f\"Results saved to: {json_path}\")\n",
    "\n",
    "print(\"\\nComparison with Baseline:\")\n",
    "baseline_clip = all_metrics.get(\"baseline\", {}).get(\"clip_content\", 0)\n",
    "for model_name, metrics in all_metrics.items():\n",
    "    if model_name != \"baseline\":\n",
    "        clip_delta = metrics[\"clip_content\"] - baseline_clip\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  CLIP-content: {metrics['clip_content']:.4f} (Δ vs baseline: {clip_delta:+.4f})\")\n",
    "        if 'clip_style' in metrics:\n",
    "            print(f\"  CLIP-style: {metrics['clip_style']:.4f}\")\n",
    "        print(f\"  Style Strength: {metrics['style_strength']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
