{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q diffusers==0.30.0 transformers accelerate safetensors datasets\n!pip install -q torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:48:29.809871Z","iopub.execute_input":"2025-11-16T13:48:29.810498Z","iopub.status.idle":"2025-11-16T13:48:36.764460Z","shell.execute_reply.started":"2025-11-16T13:48:29.810460Z","shell.execute_reply":"2025-11-16T13:48:36.763329Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# 1. Chuẩn bị subset COCO + caption chứa token mới","metadata":{}},{"cell_type":"code","source":"import json, random\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom PIL import Image\nfrom torchvision import transforms\n\nBASE_INPUT = Path(\"/kaggle/input\")\n\n# 1. Tìm file captions_train2017.json và thư mục train2017\ncaption_files = list(BASE_INPUT.rglob(\"captions_train2017.json\"))\ntrain_dirs    = list(BASE_INPUT.rglob(\"train2017\"))\n\nprint(\"Found captions files:\", caption_files)\nprint(\"Found train dirs    :\", train_dirs)\n\nif not caption_files:\n    raise FileNotFoundError(\"Không tìm thấy captions_train2017.json trong /kaggle/input\")\nif not train_dirs:\n    raise FileNotFoundError(\"Không tìm thấy thư mục train2017 trong /kaggle/input\")\n\nCAPTION_FILE  = caption_files[0]       \nTRAIN_IMG_DIR = train_dirs[0]\n\nprint(\"Using CAPTION_FILE :\", CAPTION_FILE)\nprint(\"Using TRAIN_IMG_DIR:\", TRAIN_IMG_DIR)\n\n# 2. Đọc annotation COCO\nwith open(CAPTION_FILE, \"r\") as f:\n    coco_caps = json.load(f)\n\nimages      = {img[\"id\"]: img for img in coco_caps[\"images\"]}\nannotations = coco_caps[\"annotations\"]\n\nimgid2caps = defaultdict(list)\nfor ann in annotations:\n    imgid2caps[ann[\"image_id\"]].append(ann[\"caption\"])\n\n# 3. Chọn random N ảnh làm instance images cho style <sks_style>\nN_IMAGES = 20  \nall_image_ids = list(imgid2caps.keys())\nrandom.seed(42)\nselected_ids = random.sample(all_image_ids, N_IMAGES)\n\n# 4. Tạo metadata.jsonl + ảnh 512x512\nWORK_DIR   = Path(\"/kaggle/working/sks_style_data\")\nIMG_OUT_DIR = WORK_DIR / \"images\"\nIMG_OUT_DIR.mkdir(parents=True, exist_ok=True)\n\nmetadata_path = WORK_DIR / \"metadata.jsonl\"\n\nresize_512 = transforms.Compose([\n    transforms.Resize((512, 512)),\n])\n\nwith open(metadata_path, \"w\", encoding=\"utf-8\") as fw:\n    for img_id in selected_ids:\n        img_info  = images[img_id]\n        file_name = img_info[\"file_name\"]          \n        src_path  = TRAIN_IMG_DIR / file_name\n\n        if not src_path.exists():\n            continue\n\n        # caption COCO gốc\n        caps = imgid2caps[img_id]\n        base_caption = caps[0]\n\n        # chèn token style\n        text = f\"{base_caption}, in <sks_style> style\"\n\n        # Resize về 512x512 và lưu\n        image = Image.open(src_path).convert(\"RGB\")\n        image = resize_512(image)\n        out_name = f\"{img_id}.png\"\n        out_path = IMG_OUT_DIR / out_name\n        image.save(out_path)\n\n        rec = {\n            \"file_name\": str(out_path.relative_to(WORK_DIR)),\n            \"text\": text\n        }\n        fw.write(json.dumps(rec) + \"\\n\")\n\nprint(\"✅ Done. Created subset at:\", WORK_DIR)\nprint(\" - Images:\", IMG_OUT_DIR)\nprint(\" - Metadata:\", metadata_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:48:47.633359Z","iopub.execute_input":"2025-11-16T13:48:47.634237Z","iopub.status.idle":"2025-11-16T13:53:42.203942Z","shell.execute_reply.started":"2025-11-16T13:48:47.634202Z","shell.execute_reply":"2025-11-16T13:53:42.203196Z"}},"outputs":[{"name":"stdout","text":"Found captions files: [PosixPath('/kaggle/input/d/awsaf49/coco-2017-dataset/coco2017/annotations/captions_train2017.json')]\nFound train dirs    : [PosixPath('/kaggle/input/d/awsaf49/coco-2017-dataset/coco2017/train2017')]\nUsing CAPTION_FILE : /kaggle/input/d/awsaf49/coco-2017-dataset/coco2017/annotations/captions_train2017.json\nUsing TRAIN_IMG_DIR: /kaggle/input/d/awsaf49/coco-2017-dataset/coco2017/train2017\n✅ Done. Created subset at: /kaggle/working/sks_style_data\n - Images: /kaggle/working/sks_style_data/images\n - Metadata: /kaggle/working/sks_style_data/metadata.jsonl\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# 2. Dataset class cho Textual Inversion (dùng metadata.jsonl)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass COCOTIStyleDataset(Dataset):\n    def __init__(self, data_root, tokenizer, size=512):\n        self.data_root = Path(data_root)\n        self.tokenizer = tokenizer\n        self.size = size\n        \n        self.records = []\n        meta_path = self.data_root / \"metadata.jsonl\"\n        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                self.records.append(json.loads(line.strip()))\n        \n        self.image_transform = transforms.Compose([\n            transforms.Resize((size, size), interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])  # [-1, 1]\n        ])\n    \n    def __len__(self):\n        return len(self.records)\n    \n    def __getitem__(self, idx):\n        rec = self.records[idx]\n        img_path = self.data_root / rec[\"file_name\"]\n        caption = rec[\"text\"]\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.image_transform(image)\n        \n        tokenized = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"pixel_values\": image,\n            \"input_ids\": tokenized.input_ids[0],\n            \"attention_mask\": tokenized.attention_mask[0]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:54:52.976079Z","iopub.execute_input":"2025-11-16T13:54:52.976839Z","iopub.status.idle":"2025-11-16T13:54:52.983966Z","shell.execute_reply.started":"2025-11-16T13:54:52.976813Z","shell.execute_reply":"2025-11-16T13:54:52.983187Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# 3. Load Stable Diffusion v1.5 và thêm token <sks_style>","metadata":{}},{"cell_type":"code","source":"import torch\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler\n\ndevice = \"cuda\"\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\n\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    safety_checker=None\n).to(device)\n\ntokenizer = pipe.tokenizer\ntext_encoder = pipe.text_encoder\nvae = pipe.vae\nunet = pipe.unet\nnoise_scheduler = pipe.scheduler\n\ntext_encoder.to(torch.float32)\nemb_layer = text_encoder.get_input_embeddings()\nemb_layer.weight.data = emb_layer.weight.data.to(torch.float32)\n\n# 1. Thêm token mới\nplaceholder_token = \"<sks_style>\"\nnum_added = tokenizer.add_tokens(placeholder_token)\nif num_added == 0:\n    print(\"Token đã tồn tại.\")\nplaceholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n\n# 2. Resize embedding layer\ntext_encoder.resize_token_embeddings(len(tokenizer))\nembedding_layer = text_encoder.get_input_embeddings()\n\n# 3. Khởi tạo embedding mới từ token \"painting\" (hoặc \"style\")\ninit_token = \"painting\"\ninit_token_id = tokenizer.encode(init_token, add_special_tokens=False)[0]\nwith torch.no_grad():\n    token_embeds[placeholder_token_id] = token_embeds[init_token_id].clone()\n\n# 4. Freeze toàn bộ model trừ embedding của placeholder token\nvae.requires_grad_(False)\nunet.requires_grad_(False)\ntext_encoder.requires_grad_(False)\n\nembedding_layer.weight.requires_grad_(True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:54:56.101025Z","iopub.execute_input":"2025-11-16T13:54:56.101819Z","iopub.status.idle":"2025-11-16T13:54:58.758588Z","shell.execute_reply.started":"2025-11-16T13:54:56.101794Z","shell.execute_reply":"2025-11-16T13:54:58.757883Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f491cc9c5684a2f8002eb98c935adec"}},"metadata":{}},{"name":"stderr","text":"You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([[-0.0012,  0.0368,  0.0221,  ...,  0.0158,  0.0046, -0.0219],\n        [ 0.0152,  0.0262, -0.0132,  ..., -0.0037,  0.0002,  0.0121],\n        [-0.0154, -0.0131,  0.0065,  ..., -0.0206, -0.0139, -0.0025],\n        ...,\n        [ 0.0011,  0.0032,  0.0003,  ..., -0.0018,  0.0003,  0.0019],\n        [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052],\n        [-0.0010, -0.0005,  0.0006,  ..., -0.0002, -0.0002,  0.0006]],\n       device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"# 4. Vòng lặp train Textual Inversion ","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nimport torch.nn.utils as nn_utils\n\ntrain_dataset = COCOTIStyleDataset(WORK_DIR, tokenizer, size=512)\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\nmax_train_steps = 400\nlearning_rate = 5e-5        \ngrad_accum = 4\n\noptimizer = AdamW([emb_layer.weight], lr=learning_rate)\n\nglobal_step = 0\naccum_steps = 0\nloss_history = []\n\nwhile global_step < max_train_steps:\n    for batch in train_dataloader:\n        if global_step >= max_train_steps:\n            break\n\n        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float16)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        # 1. image -> latents (half, no grad)\n        with torch.no_grad():\n            latents = vae.encode(pixel_values).latent_dist.sample()\n            latents = latents * 0.18215\n\n        # 2. add noise\n        noise = torch.randn_like(latents)\n        timesteps = torch.randint(\n            0, noise_scheduler.config.num_train_timesteps,\n            (latents.shape[0],), device=device\n        ).long()\n        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n        # 3. text encoder ở float32\n        encoder_hidden_states = text_encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )[0]                      # float32\n\n        # cast ra dtype của UNet (half)\n        encoder_hidden_states = encoder_hidden_states.to(unet.dtype)\n\n        # 4. UNet dự đoán noise (half)\n        model_pred = unet(\n            noisy_latents,\n            timesteps,\n            encoder_hidden_states\n        ).sample\n\n        target = noise\n        loss = torch.nn.functional.mse_loss(\n            model_pred.float(), target.float(), reduction=\"mean\"\n        )\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(\"Loss is NaN/Inf, stopping training.\")\n            break\n\n        loss = loss / grad_accum\n        loss.backward()\n        accum_steps += 1\n\n        if accum_steps % grad_accum == 0:\n            # chỉ cho token <sks_style> cập nhật\n            with torch.no_grad():\n                grad = emb_layer.weight.grad\n                mask = torch.ones(grad.shape[0], dtype=torch.bool, device=grad.device)\n                mask[placeholder_token_id] = False\n                grad[mask] = 0.0\n\n            # CLIP grad để tránh nổ\n            nn_utils.clip_grad_norm_([emb_layer.weight], max_norm=1.0)\n\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            global_step += 1\n            accum_steps = 0\n            loss_history.append(loss.item() * grad_accum)\n            if global_step % 50 == 0:\n                print(f\"Step {global_step}/{max_train_steps}, loss={loss_history[-1]:.4f}\")\n\n    if torch.isnan(loss) or torch.isinf(loss):\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:55:01.123021Z","iopub.execute_input":"2025-11-16T13:55:01.123813Z","iopub.status.idle":"2025-11-16T14:25:54.417214Z","shell.execute_reply.started":"2025-11-16T13:55:01.123785Z","shell.execute_reply":"2025-11-16T14:25:54.416327Z"}},"outputs":[{"name":"stdout","text":"Step 50/400, loss=0.0271\nStep 150/400, loss=0.1555\nStep 200/400, loss=0.0173\nStep 250/400, loss=0.0320\nStep 300/400, loss=0.2494\nStep 350/400, loss=0.2249\nStep 400/400, loss=0.1043\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"with torch.no_grad():\n    emb_layer = text_encoder.get_input_embeddings()\n    sks_emb = emb_layer.weight[placeholder_token_id]\n\n    # norm trung bình của toàn vocab\n    avg_norm = emb_layer.weight.detach().cpu().norm(dim=1).mean()\n    print(\"avg vocab norm:\", avg_norm.item())\n\n    # scale embedding <sks_style> lên cùng “tầm” với vocab\n    scale = avg_norm / sks_emb.norm()\n    sks_emb_scaled = sks_emb * scale\n\n    emb_layer.weight[placeholder_token_id] = sks_emb_scaled\n\n# (tuỳ chọn) lưu lại embedding đã scale\nimport pathlib, torch\nsave_dir = pathlib.Path(\"/kaggle/working/sks_style_embeddings\")\nsave_dir.mkdir(exist_ok=True, parents=True)\n\ntorch.save(\n    {\"placeholder_token\": placeholder_token,\n     \"embedding\": sks_emb_scaled.detach().cpu().unsqueeze(0)},\n    save_dir / \"sks_style_embedding_fp32_scaled.pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:26:32.486619Z","iopub.execute_input":"2025-11-16T14:26:32.486906Z","iopub.status.idle":"2025-11-16T14:26:32.590910Z","shell.execute_reply.started":"2025-11-16T14:26:32.486883Z","shell.execute_reply":"2025-11-16T14:26:32.590164Z"}},"outputs":[{"name":"stdout","text":"avg vocab norm: 0.3853093981742859\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"with torch.no_grad():\n    emb = emb_layer.weight[placeholder_token_id].detach().cpu()\n\nprint(\"min:\", emb.min().item(), \"max:\", emb.max().item())\nprint(\"has NaN:\", torch.isnan(emb).any().item())\nprint(\"has Inf :\", torch.isinf(emb).any().item())\nprint(\"norm:\", emb.norm().item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:35:31.885335Z","iopub.execute_input":"2025-11-16T14:35:31.885898Z","iopub.status.idle":"2025-11-16T14:35:31.892716Z","shell.execute_reply.started":"2025-11-16T14:35:31.885868Z","shell.execute_reply":"2025-11-16T14:35:31.892068Z"}},"outputs":[{"name":"stdout","text":"min: -0.08013193309307098 max: 0.06354348361492157\nhas NaN: False\nhas Inf : False\nnorm: 0.3853094279766083\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"# 5. Lưu checkpoint embedding","metadata":{}},{"cell_type":"code","source":"import pathlib, torch\nsave_dir = pathlib.Path(\"/kaggle/working/sks_style_embeddings\")\nsave_dir.mkdir(exist_ok=True, parents=True)\n\ntorch.save(\n    {\"placeholder_token\": placeholder_token,\n     \"embedding\": emb.unsqueeze(0)},\n    save_dir / \"sks_style_embedding_fp32.pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:26:54.074305Z","iopub.execute_input":"2025-11-16T14:26:54.074585Z","iopub.status.idle":"2025-11-16T14:26:54.080030Z","shell.execute_reply.started":"2025-11-16T14:26:54.074563Z","shell.execute_reply":"2025-11-16T14:26:54.079300Z"}},"outputs":[],"execution_count":37}]}