{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Metrics - DreamBooth vs Baseline\n",
        "\n",
        "Tính FID, LPIPS, SSIM cho DreamBooth models và so sánh với baseline.\n",
        "\n",
        "**Models đánh giá:**\n",
        "- Baseline: SD v1.5 gốc\n",
        "- DreamBooth Contemporary_Realism\n",
        "- DreamBooth New_Realism\n",
        "\n",
        "**Metrics:**\n",
        "- FID: Generated images vs Style images (WikiArt)\n",
        "- LPIPS: Generated images vs Style images (perceptual similarity)\n",
        "- SSIM: Generated images vs Content images (structure preservation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: No GPU detected!\")\n",
        "else:\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eval_utils\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionTransform\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lpips\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from torchvision import transforms as T\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchmetrics.functional.image.ssim import structural_similarity_index_measure\n",
        "\n",
        "class StableDiffusionTransform:\n",
        "    def __init__(self, size=256, center_crop=True):\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize(size),\n",
        "            T.CenterCrop(size) if center_crop else T.RandomCrop(size),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "    def __call__(self, img):\n",
        "        return self.transform(img)\n",
        "\n",
        "class FIDEvaluator:\n",
        "    def __init__(self, device=\"cpu\"):\n",
        "        self.metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
        "        self.device = device\n",
        "    def update_real(self, images):\n",
        "        images = (images * 0.5 + 0.5).clamp(0, 1)\n",
        "        self.metric.update(images.to(self.device), real=True)\n",
        "    def update_fake(self, images):\n",
        "        images = (images * 0.5 + 0.5).clamp(0, 1)\n",
        "        self.metric.update(images.to(self.device), real=False)\n",
        "    def compute(self):\n",
        "        return float(self.metric.compute())\n",
        "\n",
        "def compute_lpips(img1, img2, model):\n",
        "    img1 = (img1 * 0.5 + 0.5).clamp(0, 1)\n",
        "    img2 = (img2 * 0.5 + 0.5).clamp(0, 1)\n",
        "    return model(img1, img2).mean()\n",
        "\n",
        "def compute_ssim(img1, img2):\n",
        "    img1 = (img1 * 0.5 + 0.5).clamp(0, 1)\n",
        "    img2 = (img2 * 0.5 + 0.5).clamp(0, 1)\n",
        "    return structural_similarity_index_measure(img1, img2, data_range=1.0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "RESULTS_DIR = Path(\"/kaggle/working/results/metrics\")\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "STYLE_CHECKPOINT_MAP = {\n",
        "    \"New_Realism\": \"/kaggle/input/real1k/dreambooth_checkpoints\",\n",
        "    \"Contemporary_Realism\": \"/kaggle/input/priorimages/dreambooth_checkpoints\",\n",
        "}\n",
        "\n",
        "STYLE_IMAGES_DIRS = [\n",
        "    \"/kaggle/input/real1k/dreambooth/New_Realism/instance_images\",\n",
        "    \"/kaggle/input/priorimages/dreambooth/Contemporary_Realism/instance_images\",\n",
        "]\n",
        "\n",
        "CONTENT_IMAGES_DIRS = [\n",
        "    \"/kaggle/input/coco-2017-dataset/coco2017/val2017\",\n",
        "    \"/kaggle/input/coco2017/val2017\",\n",
        "]\n",
        "\n",
        "STYLES = [\"Contemporary_Realism\", \"New_Realism\"]\n",
        "UNIQUE_TOKEN = \"sks\"\n",
        "RESOLUTION = 256\n",
        "NUM_SAMPLES = 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading baseline model...\")\n",
        "baseline_pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False,\n",
        ")\n",
        "if torch.cuda.is_available():\n",
        "    baseline_pipeline = baseline_pipeline.to(device)\n",
        "print(\"Baseline loaded\")\n",
        "\n",
        "dreambooth_pipelines = {}\n",
        "for style_name in STYLES:\n",
        "    if style_name not in STYLE_CHECKPOINT_MAP:\n",
        "        print(f\"No checkpoint path mapped for {style_name}\")\n",
        "        continue\n",
        "    \n",
        "    checkpoint_base = Path(STYLE_CHECKPOINT_MAP[style_name])\n",
        "    checkpoint_path = checkpoint_base / style_name\n",
        "    \n",
        "    if not checkpoint_base.exists():\n",
        "        print(f\"Dataset not found: {checkpoint_base}\")\n",
        "        continue\n",
        "    \n",
        "    if not checkpoint_path.exists():\n",
        "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"Loading {style_name} from {checkpoint_base}...\")\n",
        "    try:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            str(checkpoint_path),\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            safety_checker=None,\n",
        "            requires_safety_checker=False,\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            pipeline = pipeline.to(device)\n",
        "        dreambooth_pipelines[style_name] = pipeline\n",
        "        print(f\"{style_name} loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {style_name}: {e}\")\n",
        "\n",
        "models = {\n",
        "    \"baseline\": baseline_pipeline,\n",
        "    **{f\"dreambooth_{s.lower().replace('_', '')}\": dreambooth_pipelines[s] \n",
        "       for s in STYLES if s in dreambooth_pipelines}\n",
        "}\n",
        "print(f\"\\nModels loaded: {list(models.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Reference Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "style_images = {}\n",
        "content_images = []\n",
        "\n",
        "for style_name in STYLES:\n",
        "    for style_dir_path in STYLE_IMAGES_DIRS:\n",
        "        style_dir = Path(style_dir_path)\n",
        "        if style_dir.exists() and style_name in str(style_dir):\n",
        "            style_files = list(style_dir.glob(\"*.jpg\"))[:NUM_SAMPLES]\n",
        "            if not style_files:\n",
        "                style_files = list(style_dir.glob(\"*.png\"))[:NUM_SAMPLES]\n",
        "            if style_files:\n",
        "                style_images[style_name] = [Image.open(f).convert(\"RGB\") for f in style_files]\n",
        "                print(f\"Loaded {len(style_images[style_name])} style images for {style_name} from {style_dir}\")\n",
        "                break\n",
        "\n",
        "for base_dir in CONTENT_IMAGES_DIRS:\n",
        "    content_dir = Path(base_dir)\n",
        "    if content_dir.exists() and content_dir.is_dir():\n",
        "        content_files = list(content_dir.glob(\"*.jpg\"))[:NUM_SAMPLES]\n",
        "        if content_files:\n",
        "            content_images = [Image.open(f).convert(\"RGB\") for f in content_files]\n",
        "            print(f\"Loaded {len(content_images)} content images from {content_dir}\")\n",
        "            break\n",
        "\n",
        "transform = StableDiffusionTransform(size=RESOLUTION, center_crop=True)\n",
        "\n",
        "print(f\"\\nStyle images: {sum(len(v) for v in style_images.values())} total\")\n",
        "print(f\"Content images: {len(content_images)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_prompts = [\n",
        "    f\"a {UNIQUE_TOKEN} style painting of a cat\",\n",
        "    f\"a {UNIQUE_TOKEN} style painting of a landscape\",\n",
        "    f\"a {UNIQUE_TOKEN} style painting of a portrait\",\n",
        "    f\"a {UNIQUE_TOKEN} style painting of a cityscape\",\n",
        "    f\"a {UNIQUE_TOKEN} style painting of flowers\",\n",
        "]\n",
        "\n",
        "baseline_prompts = [\n",
        "    \"a painting of a cat\",\n",
        "    \"a painting of a landscape\",\n",
        "    \"a painting of a portrait\",\n",
        "    \"a painting of a cityscape\",\n",
        "    \"a painting of flowers\",\n",
        "]\n",
        "\n",
        "generated_samples = {}\n",
        "\n",
        "print(\"Generating samples from baseline...\")\n",
        "baseline_samples = []\n",
        "for prompt in baseline_prompts:\n",
        "    image = baseline_pipeline(\n",
        "        prompt,\n",
        "        num_inference_steps=50,\n",
        "        guidance_scale=7.5,\n",
        "        height=RESOLUTION,\n",
        "        width=RESOLUTION,\n",
        "    ).images[0]\n",
        "    baseline_samples.append(image)\n",
        "generated_samples[\"baseline\"] = baseline_samples\n",
        "print(f\"Baseline: {len(baseline_samples)} samples\")\n",
        "\n",
        "for style_name in STYLES:\n",
        "    if style_name in dreambooth_pipelines:\n",
        "        model_key = f\"dreambooth_{style_name.lower().replace('_', '')}\"\n",
        "        pipeline = dreambooth_pipelines[style_name]\n",
        "        print(f\"\\nGenerating samples from {style_name}...\")\n",
        "        style_samples = []\n",
        "        for prompt in test_prompts:\n",
        "            image = pipeline(\n",
        "                prompt,\n",
        "                num_inference_steps=50,\n",
        "                guidance_scale=7.5,\n",
        "                height=RESOLUTION,\n",
        "                width=RESOLUTION,\n",
        "            ).images[0]\n",
        "            style_samples.append(image)\n",
        "        generated_samples[model_key] = style_samples\n",
        "        print(f\"{style_name}: {len(style_samples)} samples\")\n",
        "\n",
        "print(f\"\\nTotal generated samples: {sum(len(v) for v in generated_samples.values())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lpips_model = lpips.LPIPS(net=\"vgg\").to(device)\n",
        "\n",
        "def compute_metrics_for_model(\n",
        "    model_name: str,\n",
        "    generated_images: List[Image.Image],\n",
        "    style_images_list: List[Image.Image],\n",
        "    content_images_list: List[Image.Image],\n",
        ") -> Dict[str, float]:\n",
        "    transform = StableDiffusionTransform(size=RESOLUTION, center_crop=True)\n",
        "    \n",
        "    generated_tensors = torch.stack([\n",
        "        transform(img).to(device) for img in generated_images\n",
        "    ])\n",
        "    \n",
        "    if len(style_images_list) > 0:\n",
        "        style_tensors = torch.stack([\n",
        "            transform(img).to(device) for img in style_images_list[:len(generated_images)]\n",
        "        ])\n",
        "    else:\n",
        "        style_tensors = generated_tensors\n",
        "    \n",
        "    if len(content_images_list) > 0:\n",
        "        content_tensors = torch.stack([\n",
        "            transform(img).to(device) for img in content_images_list[:len(generated_images)]\n",
        "        ])\n",
        "    else:\n",
        "        content_tensors = generated_tensors\n",
        "    \n",
        "    fid_metric = FIDEvaluator(device=device)\n",
        "    fid_metric.update_real(style_tensors)\n",
        "    fid_metric.update_fake(generated_tensors)\n",
        "    fid_score = fid_metric.compute()\n",
        "    \n",
        "    lpips_scores = []\n",
        "    for gen, style in zip(generated_tensors, style_tensors):\n",
        "        lpips_score = compute_lpips(\n",
        "            gen.unsqueeze(0),\n",
        "            style.unsqueeze(0),\n",
        "            model=lpips_model,\n",
        "        )\n",
        "        lpips_scores.append(lpips_score.item())\n",
        "    lpips_mean = np.mean(lpips_scores)\n",
        "    \n",
        "    ssim_scores = []\n",
        "    for gen, content in zip(generated_tensors, content_tensors):\n",
        "        ssim_score = compute_ssim(\n",
        "            gen.unsqueeze(0),\n",
        "            content.unsqueeze(0),\n",
        "        )\n",
        "        ssim_scores.append(ssim_score.item())\n",
        "    ssim_mean = np.mean(ssim_scores)\n",
        "    \n",
        "    return {\n",
        "        \"fid\": fid_score,\n",
        "        \"lpips\": lpips_mean,\n",
        "        \"ssim\": ssim_mean,\n",
        "        \"lpips_std\": np.std(lpips_scores),\n",
        "        \"ssim_std\": np.std(ssim_scores),\n",
        "    }\n",
        "\n",
        "all_metrics = {}\n",
        "\n",
        "for model_name, samples in generated_samples.items():\n",
        "    print(f\"\\nComputing metrics for {model_name}...\")\n",
        "    \n",
        "    style_ref = style_images.get(STYLES[0], []) if \"dreambooth\" in model_name else []\n",
        "    \n",
        "    metrics = compute_metrics_for_model(\n",
        "        model_name,\n",
        "        samples,\n",
        "        style_ref,\n",
        "        content_images,\n",
        "    )\n",
        "    all_metrics[model_name] = metrics\n",
        "    print(f\"  FID: {metrics['fid']:.4f}\")\n",
        "    print(f\"  LPIPS: {metrics['lpips']:.4f} ± {metrics['lpips_std']:.4f}\")\n",
        "    print(f\"  SSIM: {metrics['ssim']:.4f} ± {metrics['ssim_std']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(all_metrics).T\n",
        "results_df.index.name = \"model\"\n",
        "results_df = results_df.reset_index()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "results_path = RESULTS_DIR / \"dreambooth_metrics.csv\"\n",
        "results_df.to_csv(results_path, index=False)\n",
        "print(f\"\\nResults saved to: {results_path}\")\n",
        "\n",
        "json_path = RESULTS_DIR / \"dreambooth_metrics.json\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "print(f\"Results saved to: {json_path}\")\n",
        "\n",
        "print(\"\\nComparison with Baseline:\")\n",
        "baseline_fid = all_metrics.get(\"baseline\", {}).get(\"fid\", 0)\n",
        "baseline_lpips = all_metrics.get(\"baseline\", {}).get(\"lpips\", 0)\n",
        "baseline_ssim = all_metrics.get(\"baseline\", {}).get(\"ssim\", 0)\n",
        "\n",
        "for model_name, metrics in all_metrics.items():\n",
        "    if model_name != \"baseline\":\n",
        "        fid_improvement = baseline_fid - metrics[\"fid\"]\n",
        "        lpips_improvement = baseline_lpips - metrics[\"lpips\"]\n",
        "        ssim_change = metrics[\"ssim\"] - baseline_ssim\n",
        "        \n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  FID: {metrics['fid']:.4f} (vs baseline: {fid_improvement:+.4f})\")\n",
        "        print(f\"  LPIPS: {metrics['lpips']:.4f} (vs baseline: {lpips_improvement:+.4f})\")\n",
        "        print(f\"  SSIM: {metrics['ssim']:.4f} (vs baseline: {ssim_change:+.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
