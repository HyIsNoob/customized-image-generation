# LoRA Training Configuration

# Base Model
base_model: "runwayml/stable-diffusion-v1-5"
model_name: "stable-diffusion-v1-5"

# LoRA Parameters
lora_rank: 4
lora_alpha: 32
lora_dropout: 0.0
target_modules:
  - "to_k"
  - "to_q"
  - "to_v"
  - "to_out.0"

# Training Parameters
learning_rate: 1.0e-4
batch_size: 2
num_epochs: 1
gradient_accumulation_steps: 1
max_train_steps: 8000
save_steps: 1000
eval_steps: 500

# Optimizer
optimizer: "AdamW"
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1.0e-8

# Scheduler
lr_scheduler: "cosine"
warmup_steps: 500

# Loss Weights
content_weight: 1.0
style_weight: 1.0
lpips_weight: 0.1

# Data
image_size: 512
content_dir: "data/coco/train"
style_dir: "data/wikiart"

# Output
output_dir: "results/lora_checkpoints"
logging_dir: "results/logs"

# Hardware
mixed_precision: "fp16"
gradient_checkpointing: true

